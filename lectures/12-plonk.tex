\documentclass[../lecture-notes.tex]{subfiles}
\usepackage[T1]{fontenc}

\pgfplotsset{compat=1.18}

\begin{document}

\subsection{Number Theoretical Trasnform: Universal Polynomial Accelerator}

In the previous sections, when considering Groth16, we have seen the idea that
polynomials are powerful encoders of data. To encode a set of $N$ values
$a_0,\dots,a_{N-1} \in \mathbb{F}$, we interpolate a polynomial $p(x)$ that at
specific points $x_0,\dots,x_{N-1} \in \mathbb{F}$ evaluates to these values. The
only condition we impose on these points is that they are distinct (unless, the
interpolation would not be properly defined). This way, generally, we have the
following interpolation problem:
\begin{equation*}
    p(x_j) = a_j, \quad j = 0,\dots,N-1
\end{equation*}

Particularly, in Groth16 our choice of points was $x_j = j$ for $j = 0,\dots,N-1$, which, 
for the large enough finite field $\mathbb{F}$, does not cause any issues. However, the 
complexity of interpolation in this case is not optimal. Let us see why.

Recall that the interpolation formula (see \Cref{section:math-crypto-2} for details)
is given by:
\begin{equation*}
    p(x) = \sum_{i=0}^{N-1} a_i \ell_i(x), \quad \ell_i(x) = \prod_{j=0, j \neq i}^{N-1} \frac{x-x_j}{x_i-x_j}.
\end{equation*}

The naive evaluation of this formula requires $O(N^2)$ operations: we
need to compute each $\ell_i$, costing $O(N)$ operations, and then sum
them up with, again, $O(N)$ operations.

With the specific choice of points $\{x_j\}_{0 \leq j < N}$, we can do much
better: in fact, we can reduce the complexity to $O(N \log N)$ operations or
even $O(N)$. This is done by utilizing several techniques, including the
\textbf{Barycentric Interpolation} and the \textbf{$N$th roots of unity}. 

\subsubsection{Barycentric Interpolation}

The idea of $O(N \log N)$ is to exploit the barycentric formula for polynomial
interpolation. Let us derive the formula and see how it helps.

First, introduce the quantity $\gamma(x) = \prod_{j=0}^{N-1}(x-x_j)$. Now note that 
the Lagrange basis polynomials $\ell_j(x)$ can be rewritten as:
\begin{equation*}
    \ell_j(x) = \gamma(x) \cdot \frac{w_j}{x-x_j}, \quad w_j = \frac{1}{\prod_{k=0,k \neq j}^{N-1}(x_j-x_k)}, \quad j \in [N]
\end{equation*}

\begin{remark}
    This step might seem unobvious, so let us be careful here. Let us see why expression $\gamma(x) \cdot \frac{w_j}{x-x_j}$ 
    indeed gives the desired basis polynomial:
    \begin{equation*}
        \textcolor{blue!60!black}{\gamma(x)} \cdot \frac{\textcolor{green!60!black}{w_j}}{x-x_j} = \textcolor{blue!60!black}{\left(\prod_{k=0}^{N-1}(x-x_k) \right)} \cdot \frac{1}{\textcolor{green!60!black}{\left(\prod_{k=0,k \neq j}^{N-1}(x_j-x_k)\right)}(x-x_j)}
    \end{equation*}

    Note that we can cancel out $(x-x_j)$ from both numerator and denominator and get
    \begin{equation*}
        \textcolor{blue!60!black}{\left(\prod_{k=0,k\neq j}^{N-1}(x-x_j) \right)} \cdot \frac{1}{\textcolor{green!60!black}{\prod_{k=0,k \neq j}^{N-1}(x_j-x_k)}} = \prod_{k=0,k \neq j}^{N-1} \frac{x-x_j}{x_j-x_k} = \ell_j(x),
    \end{equation*}
    which is exactly what we wanted to show.
\end{remark}

Good, so why do we even need such an expression for $\ell_j$? Let us substitute it back into the interpolation formula:
\begin{equation*}
    p(x) = \sum_{i=0}^{N-1} a_i \ell_i(x) = \sum_{i=0}^{N-1}a_i \gamma(x) \cdot \frac{w_i}{x-x_i} = \gamma(x) \sum_{i=0}^{N-1} \frac{w_i}{x-x_i}a_i
\end{equation*}

In regards to this formula, we give the following definition.
\begin{proposition}\label{prop:barycentric}
    The \textbf{barycentric interpolation formula} for the interpolation problem
    $p(x_j)=a_j, j \in [N]$, given by $p(x) = \gamma(x) \sum_{i \in [N]}
    \frac{w_i}{x-x_i}a_i$ with $\gamma(x)=\prod_{i \in [N]}(x-x_i)$, requires $O(N)$ operations to compute and $O(N^2)$
    operations to pre-compute.
\end{proposition}

\textbf{Proof.} Coefficients $\{w_j\}_{j \in [N]}$ are independent of $x$, and so 
are the values $\{a_j\}_{j \in [N]}$. To compute $\{w_j\}_{j \in [N]}$, one needs 
$O(N)$ operations for each $w_j$, and thus $O(N^2)$ operations in total. To compute
the polynomial $p(x)$, one needs $O(N)$ operations to compute $\gamma(x)$ and 
$O(N)$ operations to compute the sum, knowing $\{w_ja_j\}_{j \in [N]}$. $\hfill\Box$

Of course, in reality, storing $N$ values $\{w_j\}_{j \in [N]}$ requires $O(N)$
memory, which is not optimal. Moreover, these points on their own are useless 
and typically are not used in any other parts of the protocol. This is where the
\textbf{$N$th roots of unity} come into play.

\subsubsection{Multiplicative Cyclic Subgroup}\label{section:mult-cyclic-g}

Again, assume we have the prime field $\mathbb{F}_p$. Let $\omega$ be a
\textbf{primitive $N$-th root of unity}, i.e., $\omega^N = 1$ and $\omega^j \neq
1$ for $j < N$. The set $\Omega = \{\omega^j\}_{0 \le j < N}$ is called
the \textbf{$N$-th root of unity subgroup} of $\mathbb{F}_p$ of order $N$. One
might ask the following question: why such primitive root even exists? Consider
the following lemma, briefly mentioned in
\Cref{section:multiplicative_subgroup}.

\begin{lemma}
    For $\mathbb{F}_p$ there exists a primitive $N$-th root of unity
    if and only if $N \mid (p-1)$. 
\end{lemma}
What is so special about the set $\Omega$? The magic of $\Omega$ is that it
allows to compute certain polynomial operations (such as interpolation or
multiplication) using the \textbf{Discrete Fourier Transform} (DFT) or, 
equivalently, the \textbf{Number Theoretic Transform} (NTT) algorithm. 
Consider the first central lemma.

\begin{lemma}\label{lemma:vanishing-polynomial}
    The vanishing polynomial of the set $\Omega$ is given by $z_{\Omega}(X) = X^N - 1$.
\end{lemma}

\textbf{Proof Idea.} If $\omega$ is the $N$th primitive root, then for any $h
\in \Omega$ we have $h^N = 1$ and therefore all elements of $\Omega$ are the
roots of $X^N-1$. There are precisely $N$ such roots, so $X^N-1$ can be 
decomposed as a product of linear factors $c \cdot \prod_{j=0}^{N-1}(X-\omega^j)$. 
It is easy to see that $c=1$ by comparing the leading coefficient. 

Now, let us come back to the barycentric formula. We have seen that the
interpolation polynomial $p(x)$ can be written as $p(x) = \gamma(x) 
\sum_{i=0}^{N-1} \frac{w_i}{x-x_i}a_i$. The key idea of the FFT is to
set $x_j = \omega^j$. What does it give us? We give the following proposition.

\begin{proposition}
    Suppose the interpolation domain is chosen so that $a_i = \omega^i$. Then, 
    following the notation of \Cref{prop:barycentric}, certain expressions 
    simplify to the following:
    \begin{itemize}
        \item $\gamma(x) = x^N-1$.
        \item $w_i = \omega^i/N$.
    \end{itemize}
\end{proposition}

\textbf{Proof Idea.} For the first claim, notice that
by definition $\gamma(x) = \prod_{i=0}^{N-1}(x-\omega^i)$, which is exactly 
the vanishing polynomial of $\Omega$. Thus, $\gamma(x) = z_{\Omega}(x) = x^N-1$
from \Cref{lemma:vanishing-polynomial}. 

As for the second claim, recall that $w_i = 1/\prod_{j \neq
i}^{N-1}(\omega^i-\omega^j)$. Intuitively, the real analysis
shows that $w_i = 1/\gamma'(x_i)$ and since $\gamma(x)=x^N-1$, we 
have exactly $w_i = 1/Nx^{N-1}\Big|_{x=\omega^i}=\omega^i/N$. The same 
result can be obtained by direct computation. $\hfill\Box$

That being said, the barycentric formula is now given by:
\begin{equation*}
    p(x) = (x^N-1) \sum_{j \in [N]} \frac{\omega^j}{x-\omega^j}a_j
\end{equation*}

This formula is a much more convenient form for the computation of the
interpolation polynomial! First, the evaluation of the sum requires $O(N)$
operations and it depends only on the values $\{\omega^j\}_{j \in [N]}$,
which are typically pre-computed and used in many other parts of the protocol.
Second, the evaluation of the vanishing polynomial $x^N-1$ requires $O(\log N)$
operations using the fast exponentiation algorithm, compared to naive 
$O(N)$ operations. 

\subsubsection{Fast Polynomial Multiplication}

\textcolor{blue!60!black}{\textbf{Forward NTT.}} Using the $N$th roots of unity $\Omega$, we can also compute the polynomial
multiplication in $O(N \log N)$ operations. The idea is to use the \textbf{Number
Theoretic Transform} (NTT) algorithm, which is a generalization of the Fast
Fourier Transform (FFT) to the finite fields. But first, let us define 
what NTT is.

\begin{definition}[NTT]
    Suppose the polynomial is given by $p(x) = \sum_{j=0}^{N-1}p_jx^j \in \mathbb{F}[x]$. In its
    essence, the polynomial is defined as a vector of coefficients
    $\boldsymbol{p}=(p_0,\dots,p_{N-1})$. The \textbf{Number Theoretic
    Transform} (NTT) of polynomial $p(x)$ is the vector of evaluations at 
    the $N$th roots of unity $\Omega$:
    $\mathsf{NTT}(\boldsymbol{p}) =
    (p(\omega^0),p(\omega^1),\dots,p(\omega^{N-1}))$.
\end{definition}

\begin{remark}
    Typically, the $j$th component of the NTT vector is denoted as $\hat{p}_j = \mathsf{NTT}(\boldsymbol{p})_j$.
\end{remark}

If computed naively, the NTT requires $O(N^2)$ operations, since each 
evaluation of $p(x)$ requires $O(N)$ operations. However, due to the 
specifics of the selected domain $\Omega$, the NTT can be computed in
$O(N \log N)$ operations. Let us emphasize this in the following lemma.

\begin{lemma}
    The \textbf{Number Theoretic Transform} (NTT) of a polynomial $p(x)$ can be
    computed in $O(N \log N)$ operations using the $N$th roots of unity. This is
    possible only if the prime field $\mathbb{F}_p$ allows to find the primitive
    $2^k$ root of unity for $k \in [m]$ with large enough $m$. Equivalently, 
    $2^k \mid (p-1)$ for $k \in [m]$.
\end{lemma}

To show this lemma is true, let us develop the concrete algorithm. Notice that 
our task consists in computing:
\begin{equation*}
    p(\omega^i) = \sum_{j \in [N]} p_j (\omega^i)^j = \sum_{j \in [N]} p_j \omega^{ij} \; \text{for each} \; i \in [N]
\end{equation*}

Suppose the considered polynomial is such that $N=2^r$ (we can always pad the
polynomial if that is not the case). Now, let us proceed with the polynomial as
follows:
\begin{equation*}
    p(\omega^i) = \sum_{j=0}^{2^{r}-1}p_j\omega^{ij} = \sum_{j=0}^{2^{r-1}-1}p_{2j}\omega^{2ij} + \sum_{j=0}^{2^{r-1}-1}p_{2j+1}\omega^{i(2j+1)} = \sum_{j=0}^{2^{r-1}-1}p_{2j}(\omega^{2i})^j + \omega^i\sum_{j=0}^{2^{r-1}-1}p_{2j+1} (\omega^{2i})^j
\end{equation*}

This already looks interesting enough. Notice that we can introduce two new polynomials: $p_E(x)=\sum_{j=0}^{2^{r-1}-1}p_{2j}x^{j}$ and $p_O(x)=\sum_{j=0}^{2^{r-1}-1}p_{2j+1}x^{j}$, 
which are polynomials, containing even and odd coefficients of $p$, respectively. In that case, 
\begin{equation*}
    p(\omega^i) = p_E(\omega^{2i}) + \omega^i p_O(\omega^{2i})
\end{equation*}

This is quite an interesting observation which already screams
divide-and-conquer! However, currently it might still be unclear 
how to use it: we still have to evaluate $N$ expressions of 
form $p_E(\omega^{2i}) + \omega^i p_O(\omega^{2i})$ where both 
polynomials $p_E$ and $p_O$ contain roughly $N/2$ coefficients, totalling 
in $O(N^2)$ operations again. To counter this, we claim the following: 
we need only half the domain of $\Omega$ to compute both $p_E$ and $p_O$.
To see why, consider the expression $p(\omega^{i+N/2})$:
\begin{equation*}
    p(\omega^i) = p_E(\omega^{2(i+N/2)}) + \omega^{i+N/2}p_O(\omega^{2(i+N/2)}) = p_E(\omega^{2i}) + \omega^{i}\omega^{N/2}p_O(\omega^{2i})
\end{equation*}

In other words, having computed $p_E(\omega^{2i})$ and $p_O(\omega^{2i})$, we
know not only $p(\omega^i)$, but also $p(\omega^{i+N/2})$ for free! This way, to
compute the \textsf{NTT} for $N$-degree polynomial, we need to evaluate two
$\frac{N}{2}$-degree polynomials at $\frac{N}{2}$ points. This way, on each
step: (a) the evaluation domain shrinks in half, (b) the complexity of computing
polynomials shrinks in half, (c) we get two new polynomials. This way, on each
step, we reduce the problem complexity in half!

The reason why the prime field should support multiplicative cyclic subgroups 
of order $2^k$ for sufficiently many $k$ is that not always if $\omega$ is the 
$N$th primitive root, then $\omega^2$ is the $\frac{N}{2}$th primitive root. If that is 
not the case, all the aforementioned magic breaks.

We summarize everything so far in the \Cref{alg:ntt}.

\begin{algorithm}[H]
    \caption{Number Theoretic Transform (NTT)}
    \label{alg:ntt}
    \Input{Polynomial $p(x) = \sum_{j=0}^{N-1}p_jx^j$}
    \Output{Vector of evaluations $\mathsf{NTT}(\boldsymbol{p}, \omega)$ at $\Omega=\{\omega\}_{j \in [N]}$}

    \If{$N=1$}{
        \Return{$(p_0)$}
    }

    $H \gets N/2$ \Comment{Compute the domain half-size}

    $\boldsymbol{p}_{E} \gets (p_0,p_2,\dots,p_{N-2})$ \Comment{Find even-indexed coefficients}

    $\boldsymbol{p}_{O} \gets (p_1,p_3,\dots,p_{N-1})$ \Comment{Find odd-indexed coefficients}

    $\boldsymbol{y}_E \gets \mathsf{NTT}(\boldsymbol{p}_{E}, \omega^2)$ \Comment{Compute NTT for even polynomial via $\frac{N}{2}$th primitive root $\omega^2$}

    $\boldsymbol{y}_O \gets \mathsf{NTT}(\boldsymbol{p}_O, \omega^2)$ \Comment{Compute NTT for odd polynomial via $\frac{N}{2}$th primitive root $\omega^2$}

    \Return{$(y_0,\dots,y_{N-1})$ with $y_j=y_{E, \;j \; \text{mod} \; H} + \omega^j y_{O, \; j \; \text{mod} \; H}$}
\end{algorithm}

\textcolor{purple}{\textbf{NTT Domain.}} OK, so what next? Suppose we want to
multiply two polynomials $p(x), q(x) \in \mathbb{F}[X]$ of degree $N=2^r$ and we
have successfully evaluated their NTTs. Say, we got $\hat{\boldsymbol{p}}$ and
$\hat{\boldsymbol{q}}$. What can we do next? Here is another trick.

\begin{proposition}
    Suppose $m(x) = p(x)q(x)$ is the product of $p$ and $q$. Then,
    \begin{equation*}
        \hat{\boldsymbol{m}} = \hat{\boldsymbol{p}} \odot \hat{\boldsymbol{q}}
    \end{equation*}

    Speaking more formally, $\mathsf{NTT}: (\mathbb{F}^{(\leq N)}[X],\times) \to (\mathbb{F}^N, \odot)$ is 
    a homomorphism between a set of polynomials of degree up to $N$ and their NTT domain. With certain 
    appropriate technicalities, $\mathsf{NTT}$ can be extended to the isomorphism.
\end{proposition}

\textbf{Intuition.} Although this fact might come out of random, we give an
intuitive explanation why this holds. One of NTT interpretations is well-known to you 
\emph{interpolation}. Indeed, polynomials $p$ and $q$ satisfy the following 
interpolation problem:
\begin{equation*}
    p(\omega^j) = \mathsf{NTT}(p)_j = \hat{p}_j, \quad q(\omega^j) = \mathsf{NTT}(q)_j = \hat{q}_j.
\end{equation*}

In turn, $\hat{m}_j$ is nothing but the evaluation of $m$ at $\omega^j$. But, if $m$ is the product of $p$ and $q$
and values of $p$ and $q$ at $\omega^j$ are $\hat{p}_j$ and $\hat{q}_j$, this immediately implies that 
$m(\omega^j)$ is nothing but $\hat{p}_j\hat{q}_j$. Meaning, $\hat{m}_j = \hat{p}_j\hat{q}_j$. This, in turn,
implies $\hat{\boldsymbol{m}} = \hat{\boldsymbol{p}} \odot \hat{\boldsymbol{q}}$.

Wow! What this essentially means is that multiplication in NTT domain is very cheap:
it requires only $O(N)$ field multiplication operations! Now, the algorithm 
of multiplication of $p$ and $q$ becomes a little bit more clear at this point:
\begin{enumerate}
    \item Compute NTTs $\hat{\boldsymbol{p}}$ and $\hat{\boldsymbol{q}}$ of $p$ and $q$.
    \item Compute NTT $\hat{\boldsymbol{m}} = \hat{\boldsymbol{p}} \odot \hat{\boldsymbol{q}}$ of their product $m = pq$.
    \item Restore $m(x)$ from NTT $\hat{\boldsymbol{m}}$ --- this problem is called Inverse NTT (INTT).
\end{enumerate}

\textcolor{green!60!black}{\textbf{Inverse NTT.}} So, the only problem left is restoring the polynomial from 
its NTT form. This problem is equivalent to solving the interpolation problem:
\begin{equation*}
    m(\omega^j) = \hat{m}_j, \quad j \in [N]
\end{equation*}

This problem was extensively discussed in \Cref{section:mult-cyclic-g}, where 
it was shown that the complexity of such problem is $O(N \log N)$.

To summarize, consider the \Cref{fig:ntt}.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=2.5cm, >=Stealth]
    % Zones
    \node[very thick, draw=blue, fill=blue, fill opacity=0.1,minimum width=9.5cm, minimum height=1.75cm,dashed,on background layer](targetbox) at (2.25, 0.1) {};
    \node(targetzonelabel)[above=0.6cm of targetbox,anchor=north,color=blue!90!black]{\textbf{Polynomial Space}};

    \node[very thick, draw=green, fill=green!80!black, fill opacity=0.1,minimum width=9.5cm, minimum height=2.0cm,dashed,on background layer](targetbox) at (2.25, -2.0) {};
    \node(targetzonelabel)[below=0.15cm of targetbox,anchor=north,color=green!60!black]{\textbf{NTT Space}};

    % Nodes
    \node (pq) at (0,0) {\( p, q \)};
    \node (m) [right=3cm of pq] {\( m = p \cdot q \)};
    \node (pHatqHat) [below=2cm of pq] {\( \hat{p}, \hat{q} \)};
    \node (mHat) [below=2cm of m] {\( \hat{m} = \hat{p} \odot \hat{q} \)};
    
    % Arrows
    \draw[->, line width = 0.5mm] (pq) -- (m) node[midway, above] {$O(N^2)$};
    \draw[->, dashed, line width = 0.5mm] (pq) -- (pHatqHat) node[midway, left] {$O(N \log N)$};
    \draw[->, dashed, line width = 0.5mm] (pHatqHat) -- (mHat) node[midway, above] {$O(N)$};
    \draw[->, dashed, line width = 0.5mm] (mHat) -- (m) node[midway, right] {$O(N \log N)$};

    % Labels
    \node at ($(pq)!0.5!(pHatqHat)$) [left] {};
    
    \end{tikzpicture}
\caption{Illustration of the NTT Algorithm}
\label{fig:ntt}
\end{figure}

\subsection{Plonk Arithmetization}

Consider we have a certain relation $\mathcal{R}$, which we would like to write
down into a processing-prone format over the field $\mathbb{F}$. Plonk arithmetizes this relation into a set
of \textit{8 polynomials}, which are then used to verify the witness knowledge. Let us 
start with the concrete example.

\begin{example}
To begin with, observe this fairly simple relation
$\mathcal{R}_{\text{example}}$: suppose we have a public input $x \in
\mathbb{F}$ and public output $y \in \mathbb{F}$, and we want to prove the knowledge of $e \in
\mathbb{F}$ such that $e \times x + x - 1 = y$. Formally, we have the following relation:
\begin{equation*}
    \mathcal{R}_{\text{example}} = \left\{ \begin{matrix}
        \textbf{Public Statement:} & x, y \in \mathbb{F} \\
        \textbf{Witness}: & e \in \mathbb{F}
    \end{matrix} \; \Big| \; e \times x + x - 1 = y \right\}
\end{equation*}

\end{example}

\begin{remark}
    Note that of course, from $x$ and $y$, it is fairly simple to find $e$: simply take $\frac{1 - x + y}{x}$. However, the Plonk arithmetization 
    is not limited to this simple example, and can be applied to more complex relations, such as hash function pre-image knowledge or any NP statement.
\end{remark}

\subsubsection{Execution Trace}

Standard Plonk is defined as a system with two types of gates: \textbf{addition}
and \textbf{multiplication}. We would explain how to build custom gates later.
So, let us consider our program in terms of gates with left, right operands and
output. 

\begin{example}
    We need \textbf{three gates} to encode our program:
    \begin{enumerate}
        \item \textbf{Gate \#1}: left $e$, right $x$, output \(u = e \times x\)
        \item \textbf{Gate \#2}: left $u$, right $x$, output \(\upsilon = u + x\)
        \item \textbf{Gate \#3}: left $\upsilon$, right $x$, output \(w = \upsilon + (-1)\)
    \end{enumerate}
\end{example}

You might have glanced the intuitive formation of what is called
\textit{execution trace table} --- a matrix $T$ with columns $L$, $R$ and $O$
(it is common to denote those as $A, B, C$ to distinguish from another matrix we
will discuss later). Moreover, we will mark columns $\mathbf{A}$, $\mathbf{B}$ and
$\mathbf{C}$ in bold to indicate that they are vectors from $\mathbb{F}^N$, where 
here and hereafter, unless stated otherwise, $N$ is the number of gates in the program.

\begin{example}
We might visualize the execution trace table $T$ for the example program as follows:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!30}$\mathbf{A}$ & $\mathbf{B}$ & $\mathbf{C}$ \\ \hline
2 & 3 & 6 \\ \hline
6 & 3 & 9 \\ \hline
9 & \xmark & 8 \\ \hline
\end{tabular}
\end{center}

Notice how the last row has no value in $\mathbf{B}$ column (marked by \xmark)
--- this is reasoned by the fact it is not a variable, but rather a constant,
meaning it doesn't depend on execution. Also note that the number of gates in 
this particular circuit is $N=3$.
\end{example}

\begin{remark}
    As you might notice, in contrast to classic R1CS (which we used for Groth16), the standard Plonk arithmetization as is
    only allows two input values to be processed at a time. This way, if Groth16 requires only one constraint 
    for verifying $x_1(x_2+x_3+x_4) = x_5$, Plonk would need three constraints to verify the same statement. 
    Custom gates partially solve this problem as we will see later, but it is important to keep in mind.
\end{remark}

\subsubsection{Encode the program}

It is essential to distinguish the definition of the program and its specific
evaluation for the sake of simplicity and efficiency --- once having established
encoding for the program, you might apply it for any reasonable inputs.
Therefore, let us at first focus on what defines whether execution trace table
will be considered valid for our circuit, because having a table by itself does
not tell much, since it can be populated with any values. 

For that reason, we would define two matrices --- $Q \in \mathbb{F}^{N \times
5}$ and $V \in \mathbb{Z}_{\geq 0}^{N \times 3}$ where $N \in \mathbb{N}$, again, is the number of gates in the
program:
\begin{itemize}
    \item $Q$ is the \textbf{Gate Matrix}, which encodes the values of the gates and stores all the intermediate values computed.
    \item $V$ is the \textbf{Wiring Matrix}, which encodes the wiring of the gates, i.e., how the output of one gate is carried as input to another.
\end{itemize}

\begin{definition}
The \textbf{gate matrix} $Q \in \mathbb{F}^{N \times 5}$ has one row per each gate with columns $\mathbf{Q}_L$,
$\mathbf{Q}_R$, $\mathbf{Q}_O$, $\mathbf{Q}_M$, $\mathbf{Q}_C$ from
$\mathbb{F}^N$. If columns $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C} \in
\mathbb{F}^N$ of the execution trace table form valid evaluation of the circuit, 
then the following holds:
\begin{equation*}
    A_i (Q_{L})_i + B_i (Q_{R})_i + A_i B_i (Q_{M})_i + C_i (Q_{O})_i + (Q_{C})_i = 0, \; \forall i \in [N]    
\end{equation*}

Using Hadamard product notation, this can be concisely rewritten as:
\begin{equation*}
    \mathbf{A} \odot \mathbf{Q}_L + \mathbf{B} \odot \mathbf{Q}_R + \mathbf{A} \odot \mathbf{B} \odot \mathbf{Q}_M + \mathbf{C} \odot \mathbf{Q}_O + \mathbf{Q}_C = \mathbf{0}
\end{equation*}

\end{definition}

\begin{example}

For our program, we would have a following $Q$ table:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor{gray!30}$\mathbf{Q}_L$ & $\mathbf{Q}_R$ & $\mathbf{Q}_M$ & $\mathbf{Q}_O$ & $\mathbf{Q}_C$ \\ 
\hline
\textcolor{blue}{0} & \textcolor{blue}{0} & \textcolor{blue}{1} & \textcolor{blue}{$-1$} & \textcolor{blue}{0} \\ 
\hline
\textcolor{blue}{1} & \textcolor{blue}{1} & \textcolor{blue}{0} & \textcolor{blue}{$-1$} & \textcolor{blue}{0} \\ 
\hline
\textcolor{blue}{1} & \textcolor{blue}{0} & \textcolor{blue}{0} & \textcolor{blue}{$-1$} & \textcolor{blue}{$-1$} \\ 
\hline
\end{tabular}
\end{center}

You can verify that our claim holds for aforementioned trace matrix:
\begin{align*}
  &2 \times \textcolor{blue}{0} + 3 \times \textcolor{blue}{0} + 2 \times 3 \times \textcolor{blue}{1} + 6 \times \textcolor{blue}{(-1)} + \textcolor{blue}{0} = 0 \\
  &6 \times \textcolor{blue}{1} + 3 \times \textcolor{blue}{1} + 6 \times 3 \times \textcolor{blue}{0} + 9 \times \textcolor{blue}{(-1)} + \textcolor{blue}{0} = 0 \\
  &9 \times \textcolor{blue}{1} + 0 \times \textcolor{blue}{0} + 9 \times 0 \times \textcolor{blue}{0} + 8 \times \textcolor{blue}{(-1)} + \textcolor{blue}{(-1)} = 0 
\end{align*}

Recall that columns of trace matrix $T$ are $\mathbf{A} = \begin{bmatrix} 2 \\ 6 \\ 9 \end{bmatrix}, \;\mathbf{B} = \begin{bmatrix} 3 \\ 3 \\ \text{\xmark} \end{bmatrix}, \;\mathbf{C} = \begin{bmatrix} 6 \\ 9 \\ 8 \end{bmatrix}$.

\end{example}

Now, we do have a way of encoding gates separately, yet in order to guarantee how result of one gate is carried in as input of the other (\textit{wirings}), we need another matrix --- $V$.

\begin{definition}
The \textbf{wiring matrix} $V \in \mathbb{Z}_{\geq 0}^{N \times 3}$ consists of indices of all inputs and intermediate values, so that if $T$ is a valid trace,
\[\forall (i, j) \; \forall (k, \ell): \; V_{i,j} = V_{k,\ell} \implies T_{i,j} = T_{k,\ell}\]

Put more simply, if two values are equal in $V$, then the corresponding values (corresponding to these indices) in $T$ must be equal as well.
\end{definition}

\begin{example}
For our program, $V$ can be defined as follows:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!30}$\mathbf{L}$ & $\mathbf{R}$ & $\mathbf{O}$ \\
\hline
0 & 1 & 2 \\
\hline
2 & 1 & 3 \\
\hline
3 & \xmark & 4 \\
\hline
\end{tabular}
\end{center}
Here $0$ is an index of $e$, $1$ is an index of $x$, 2 --- of intermediate value $u$, 3 --- of $\upsilon$ and finally $4$ --- of output $w$.
\end{example}

\subsubsection{Custom Gates}
In order to reach beyond classical operations such as addition and multiplication, one may consider composing a custom gate. The main streamliner of this functionality is a matrix $Q$, using $5$ basic columns of which, you already may build custom logic.

\begin{example}
Our entire program may be encoded as one custom gate.
\begin{center}
$Q=$ 
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor{gray!30}$Q_L$ & $Q_R$ & $Q_M$ & $Q_O$ & $Q_C$ \\ 
\hline
\textcolor{blue}{0} & \textcolor{blue}{1} & \textcolor{blue}{1} & \textcolor{blue}{$-1$} & \textcolor{blue}{$-1$} \\ 
\hline
\end{tabular}
\quad $V=$
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!30}L & R & O \\
\hline
0 & 1 & 2 \\
\hline
\end{tabular}
\quad $T=$
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!30}A & B & C \\
\hline
\textcolor{purple}{2} & \textcolor{purple}{3} & \textcolor{purple}{8} \\
\hline
\end{tabular}

\[\textcolor{purple}{2} \times \textcolor{blue}{0} + \textcolor{purple}{3} \times \textcolor{blue}{1} + \textcolor{purple}{2} \times \textcolor{purple}{3} \times \textcolor{blue}{1} + \textcolor{purple}{8} \times \textcolor{blue}{(-1)} + \textcolor{blue}{(-1)} = 0\]
\end{center}
As you can see, custom gates is a good way to reduce the number of constraints needed for the same program.
\end{example}

\begin{remark}
Real-world PlonK applications commonly have additional columns in the $Q$ matrix, enabling an even broader set of custom functionality.
\end{remark}

\subsubsection{Public Inputs}
With the current design, we can prove that the computations were done correctly,
but we have no restrictions on the values of inputs. For example, when the
prover wants to convince the verifier that he knows $e$ for $x=3$ and $y=7$, the
verifier does not even check whether $x$ is $3$ (not to mention whether the
result of execution $y=7$ is correct) in the trace table $T$. One way of doing
this is by incorporating them in three previously defined matrices $Q$, $V$,
$T$.

\begin{proposition}

One way to solve this is to use the \textbf{equality gates}. Introduce two gadgets:
\begin{itemize}
    \item \textbf{Constant Equality Gate:} Suppose we want to check whether the
    certain variable equals to the constant value $\alpha \in \mathbb{F}$ at
    gate with index $i$. For $i$th gate, set $(Q_L)_i = -1$, $(Q_C)_i = \alpha$
    and other columns to $0$. Then, add a row to $V$ with $L = i$, $R =
    \text{\xmark}$ and $O = \text{\xmark}$. Then, to satisfy the condition, the
    $i$th left input \textbf{must} be equal to $\alpha$.
    \item \textbf{Nodes Equality Gate:} Suppose we want to check whether the
    $i$th and $j$th gates have equal outputs in the $k$th gate. Set $(Q_L)_k = 1$, $(Q_R)_k = -1$ 
    with other columns to $0$. Add a row to $V$ with $L =
    i$, $R = j$ and $O = \text{\xmark}$. Then, to satisfy the condition, the
    $i$th and $j$th outputs \textbf{must} be equal.
\end{itemize}

\end{proposition}

\begin{example}
Suppose the prover wants to prove that he knows $e$ for the public statement $(x,y) = (3,8)$. We can encode this
as follows:
\begin{center}
$Q=$ 
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor{gray!30} $\mathbf{Q}_L$ & $\mathbf{Q}_R$ & $\mathbf{Q}_M$ & $\mathbf{Q}_O$ & $\mathbf{Q}_C$ \\ 
\hline
$-1$ & 0 & 0 & 0 & 3 \\ 
\hline
$-1$ & 0 & 0 & 0 & 8 \\ 
\hline
1 & 1 & 1 & $-1$ & 1 \\ 
\hline
1 & $-1$ & 0 & 0 & 0 \\ 
\hline
\end{tabular}
\quad $V=$
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!30} $\mathbf{L}$ & $\mathbf{R}$ & $\mathbf{O}$ \\
\hline
0 & \xmark & \xmark \\
\hline
1 & \xmark & \xmark \\
\hline
2 & 0 & 3 \\
\hline
1 & 3 & \xmark \\
\hline
\end{tabular}
\quad $T=$
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!30} $\mathbf{A}$ & $\mathbf{B}$ & $\mathbf{C}$ \\
\hline
3 & \xmark & \xmark \\
\hline
8 & \xmark & \xmark \\
\hline
2 & 3 & 8 \\
\hline
8 & 8 & \xmark \\
\hline
\end{tabular}
\end{center}

As can be seen, besides the original program gate, inscribed in the third row, we have three additional gates:
\begin{itemize}
    \item The first two gates ``allocate'' two nodes with indices $0$ and $1$ to the values $3$ and $8$ respectively. This is done through the \emph{constant equality gates}.
    \item The last gate checks whether the result of the third gate is equal to the index $1$, corresponding to the allocated value $8$. This is done through the \emph{nodes equality gate}.
\end{itemize}
\end{example}

The primary problem with this approach, is that now we have lost agnosticism in
$Q$ and $V$ of concrete evaluations. In other words, our circuit is now
``hardcoded'' to the specific values of public inputs. In order to resolve this,
we would define a separate one-column matrix named $\boldsymbol{\Pi} \in \mathbb{F}^N$, in which
we would encode the public inputs.

\begin{example}
With only $Q$ modified, we now have:

\centering 
\begin{center}
\begin{tabular}{|c|}
\hline
\rowcolor{gray!30} $\boldsymbol{\Pi}$ \\ 
\hline
3 \\ 
\hline
8 \\ 
\hline
0 \\ 
\hline
0 \\ 
\hline
\end{tabular} \hspace{1cm}
$Q=$ 
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor{gray!30} $\mathbf{Q}_L$ & $\mathbf{Q}_R$ & $\mathbf{Q}_M$ & $\mathbf{Q}_O$ & $\mathbf{Q}_C$ \\ 
\hline
$-1$ & 0 & 0 & 0 & 0 \\ 
\hline
$-1$ & 0 & 0 & 0 & 0 \\ 
\hline
1 & 1 & 1 & $-1$ & 1 \\ 
\hline
1 & $-1$ & 0 & 0 & 0 \\ 
\hline
\end{tabular}
\end{center}
\end{example}

\begin{proposition}[Wrap-up]\label{prop:plonk-conditions}
The matrix $T$ with columns $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C} \in \mathbb{F}^N$ encodes correct execution of the program, if the following two conditions hold:
\begin{enumerate}
    \item \(\forall i \in [N]: A_i (Q_{L})_i + B_i(Q_{R})_i + A_i B_i (Q_{M})_i + C_i (Q_{O})_i + (Q_{C})_i + \Pi_i = 0\)
    \item \(\forall (i, j) \, \forall (k, \ell): V_{i,j} = V_{k,\ell} \implies T_{i,j} = T_{k,\ell}\)
\end{enumerate}
\end{proposition}

\subsubsection{Matrices to Polynomials}

\textcolor{blue!80!black}{\textbf{Gates Satisfability}.} Now we can traduce the
sets of constraints on matrices to just a few equations on polynomials, as we
have already done for Groth16. Let $\omega$ be a primitive $N$-th root of
unity\footnote{Suppose such $\omega$ exists, then $\omega^N = 1$ and $\omega^j
\neq 1$ for $0 \le j < N$.} and let $\Omega = \{\omega^j\}_{0 \le j < N}$.
Although currently the choice of set $\Omega$ might seem totally random, in the
next sections we will see how the usage of Fast-Fourier Transform (FFT) will
make this choice convenient.

Let $a, b, c, q_L, q_R, q_M, q_O, q_C, \pi \in \mathbb{F}^{(\leq N)}[X]$ be
polynomials of degree at most $N$ that interpolate corresponding columns from
matrices at the domain $\Omega$. In other words, we have \(\forall j \in [N]:
a(\omega^j) = A_j\) and the same holds for other polynomials.

Notice that if our trace matrix is correct, then the first condition of
\Cref{prop:plonk-conditions} can be reduced to the following polynomial
equation:
\begin{equation*}
    a(\omega^j)q_L(\omega^j) + b(\omega^j)q_R(\omega^j) + a(\omega^j)b(\omega^j)q_M(\omega^j) + c(\omega^j)q_O(\omega^j) + q_C(\omega^j) + \pi(\omega^j) = 0, \; \forall j \in [N]
\end{equation*}

Notice that this essentially means that the left polynomial $aq_L + bq_R + abq_M
+ cq_O + q_C + \pi$ has roots at $\omega^j$ for all $j \in [N]$. This is
equivalent to stating that the vanishing polynomial of $\Omega$ divides the left
hand side. Due to the proposition \Cref{prop:vanishing-polynomial}, this
vanishing polynomial is $X^N-1$.  Let us wrap this up in the following
proposition.

\begin{proposition}
    Now we can reduce down our first condition of \Cref{prop:plonk-conditions}
    to checking valid execution trace into the following claim over polynomials:
    \[\exists t \in \mathbb{F}^{(\leq 3N)}[X]: \; aq_L + bq_R + abq_M + cq_O + q_C + \pi = z_{\Omega}t,\]
    where $z_{\Omega}(X)$ is the vanishing polynomial $X^N - 1$.
\end{proposition}

\textcolor{purple}{\textbf{Wiring Satisfability}.} The next step is to shrink the second condition imposed by the $V$ matrix. This
may be achieved by introducing the concept of permutation. 

\begin{remark}
Permutation of the set $S$ is commonly denoted as $\sigma: S \to S$. This function is bijective, meaning that for every $s \in S$ there exists a unique $s' \in S$ such that $\sigma(s) = s'$.
\end{remark}

\begin{example}
A permutation is a rearrangement of the set, which is in our case: 
\begin{equation*}
    \mathcal{I} = \{(i, j) : \text{such that } 0 \leq i < N, \text{ and } 0 \leq j
< 3\}    
\end{equation*}

Naturally, the matrix $V$ induces a permutation $\sigma$ of this set where
$\sigma((i,j))$ equals to the pair of indices of the next occurrence of the
value at position $(i,j)$. So, for our example:

\begin{center}
\quad $V=$
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!30} $\mathbf{L}$ & $\mathbf{R}$ & $\mathbf{O}$ \\
\hline
\textcolor{green!60!black}{\textbf{0}} & \xmark & \xmark \\
\hline
1 & \xmark & \xmark \\
\hline
2 & \textcolor{green!60!black}{\textbf{0}} & 3 \\
\hline
1 & 3 & \xmark \\
\hline
\end{tabular}
\end{center}

We have the following permutation:
\[\sigma(\textcolor{green!60!black}{\textbf{(0,0)}}) = \textcolor{green!60!black}{\textbf{(2,1)}},\; \sigma((0,1)) = (0,3),\; \sigma((0,2)) = (0,2)\]
\[\sigma((0,3)) = (0,1),\; \sigma((2,1)) = (0,0),\; \sigma((3,1)) = (2,2)\]

For demonstration purposes, we marked in
\textcolor{green!60!black}{\textbf{green}} the index of the first and second
occurance of the value $0$. For proper $\sigma$ definition (as it has to be bijective), 
the application of $\sigma$ to the last occurance outputs the first one.

\end{example}

\textcolor{green!60!black}{\textbf{Permutation Check.}} This is probably the most
tedious part of PlonK. We split the following derivation into two parts:
\begin{itemize}
    \item \textbf{Set Equality using Polynomials.} We will show how to check
    whether two sets of field elements are equal using polynomials.
    \item \textbf{Permutation Check using Polynomials.} We will show how to
    check whether a given function is a permutation using polynomials in several 
    forms.
\end{itemize}

\textbf{Set equality.} Having defined permutation, we can now reduce the second
condition of \Cref{prop:plonk-conditions} of valid execution trace matrix into
the following check:
\[\forall (i, j) \in \mathcal{I}: T_{i,j} = T_{\sigma(i,j)}\]

You may have noticed how this can be reformulated as equality of two sets $A$ and $B$:
\begin{align*}
    A &:= \{((i, j), T_{i,j}) : (i, j) \in \mathcal{I}\} \\
    B &:= \{(\sigma((i, j)), T_{i,j}) : (i, j) \in \mathcal{I}\}
\end{align*}

We can reduce this check down to polynomial equations! Here is how: suppose for
simplicity we have two sets with two elements \(A = \{a_0, a_1\}\) and \(B =
\{b_0, b_1\}\). Introduce two sets of polynomials \(A' = \{a_0 + X, a_1 +
X\}$ and $B' = \{b_0 + X, b_1 + X\}\). 

When do we have the set equality \(A' = B'\)? Well, \((a_0 + X)(a_1 + X) = (b_0
+ X)(b_1 + X)\) works fine. This is true because of linear polynomial unique
factorization property, working as prime factors. Now, we can utilize
Schwartz-Zippel lemma to replace the latter formula with \((a_0 + \gamma)(a_1 +
\gamma) = (b_0 + \gamma)(b_1 + \gamma)\) for some random $\gamma \xleftarrow{R}
\mathbb{F}$ with overwhelming probability, being at least $1-2/|\mathbb{F}|$. If we
wish to generalize this for arbitrary sets \(A = \{a_0, \ldots, a_{k-1}\}\) and
\(B = \{b_0, \ldots, b_{k-1}\}\), apply the following equivalent check:

\[\prod_{i=0}^{k-1} (a_i + \gamma) = \prod_{i=0}^{k-1} (b_i + \gamma)\]

Let $\Omega$ be a domain of the form \(\{1, \omega, \dots, \omega^{k-1}\}\) for
some $k$-th root of unity $\omega$. Let $f$ and $g$ be polynomials that we
interpolate at $\Omega$ as follows:

\begin{equation*}
    f(\omega^j) = a_j + \gamma, \quad g(\omega^j) = b_j + \gamma, \quad j \in [k]
\end{equation*}

Then, \(\prod_{i=0}^{k-1} (a_i + \gamma) = \prod_{i=0}^{k-1} (b_i + \gamma)\) holds if and only if there is a polynomial $Z \in \mathbb{F}[X]$ such that for all $h \in \Omega$ we have $Z(\omega^{0}) = 1$ and $Z(h)f(h) = g(h)Z(\omega h)$.

Now that we can encode equality of sets of field elements, let's expand this to sets of tuples of field elements. Let \(A = \{(a_0, a_1), (a_2, a_3)\}\) and \(B = \{(b_0, b_1), (b_2, b_3)\}\). Then, similarly, if
\[A' = \{a_0 + a_1Y + X, a_2 + a_3Y + X\}, \quad B' = \{b_0 + b_1Y + X, b_2 + b_3Y + X\},\]

then $A=B$ if and only if $A'=B'$. As before, we can leverage Schwartz-Zippel lemma to reduce this down into sampling two random $\beta$ and $\gamma \xleftarrow{R} \mathbb{F}$ and checking equality of:
\begin{equation*}
    (a_0 + \beta a_1 + \gamma)(a_2 + \beta a_3 + \gamma) = (b_0 + \beta b_1 + \gamma)(b_2 + \beta b_3 + \gamma)
\end{equation*}

\textbf{Permutation Check.} Now, to go back to the second condition of
\Cref{prop:plonk-conditions} which we are trying to formulate in the polynomial
domain, it becomes clear that if we somehow encoded inner indices tuple $(i, j)$
into a one field element, we could use the above fact. Recall that $i \in [N]$
and $j \in \{0,1,2\}$. Thus, take the $3N$-th primitive root of unity $\eta$ and
define the bijective map $((i,j),v) \mapsto (\eta^{3i+j}, T_{i,j})$. Thus,
consider the modified sets:
\begin{align*}
    A &= \{(\eta^{3i+j}, T_{i,j}) : (i, j) \in \mathcal{I}\} \\
    B &= \{(\eta^{3k+\ell}, T_{i,j}) : (i, j) \in \mathcal{I}, \sigma((i, j)) = (k, \ell)\}
\end{align*}

Sample two random field elements $\beta$ and $\gamma \xleftarrow{R} \mathbb{R}$. Let $\mathcal{D} = \{1, \eta, \eta^2, \ldots, \eta^{3N-1}\}$. Then, interpolate two polynomials $f$ and $g$ over the defined set $\mathcal{D}$ as follows:
\begin{align*}
    f(\eta^{3i+j}) &= T_{i,j} + \eta^{3i+j}\beta + \gamma, \quad (i, j) \in \mathcal{I} \\
    g(\eta^{3k+\ell}) &= T_{i,j} + \eta^{3k+\ell}\beta + \gamma, \quad (i, j) \in \mathcal{I}, \quad \sigma((i, j)) = (k, \ell)
\end{align*}

Similarly to our previous discussion, there should be a polynomial $Z \in
\mathbb{F}[X]$ such that $\forall d \in \mathcal{D}$, we have $Z(\eta^{0}) = 1$ and
$Z(d)f(d) = g(d)Z(\eta d)$. This would imply the set equality $A=B$ with
overwhelming probability according to Schwartz-Zippel lemma.

\textbf{Shorter Form.} Now, using the $3N$-th root of unity is a bit of overkill,
so let us try compressing it down to $\Omega = \{\omega^j\}_{0 \leq j < N}$
where $\omega$ is the $N$th root of unity. We will define three polynomials
$S_{\sigma,1}$, $S_{\sigma,2}$, $S_{\sigma,3} \in \mathbb{F}[X]$, which are
interpolated as follows:
\begin{align*}
    S_{\sigma,1}(\omega^i) &= \eta^{3k+\ell}, \quad (i, 0) \in \mathcal{I}, \quad \sigma((i, 0)) = (k, \ell) \\
    S_{\sigma,2}(\omega^i) &= \eta^{3k+\ell}, \quad (i, 1) \in \mathcal{I}, \quad \sigma((i, 1)) = (k, \ell) \\
    S_{\sigma,3}(\omega^i) &= \eta^{3k+\ell}, \quad (i, 2) \in \mathcal{I}, \quad \sigma((i, 2)) = (k, \ell)
\end{align*}

Let $k_1$ and $k_2$ be two field elements such that $\omega^i \neq \omega^j k_1
\neq \omega^{\ell} k_2$ for all possible triplets $i, j, \ell$. Recall that
$\beta$ and $\gamma$ are random field elements. Let $f$ and $g$ be the
polynomials that interpolate, respectively, the following values at $\Omega$:
\begin{align*}
    f(\omega^i) &=\left(T_{i,0} + \omega^i \beta + \gamma\right)\left(T_{i,1} + \omega^i k_1 \beta + \gamma\right)\left(T_{i,2} + \omega^i k_2 \beta + \gamma\right), \quad i \in [N] \\
    g(\omega^i) &= \left(T_{i,0} + S_{\sigma,1}(\omega^i) \beta + \gamma\right)\left(T_{i,0} + S_{\sigma,2}(\omega^i) \beta + \gamma\right)\left(T_{i,0} + S_{\sigma,3}(\omega^i) \beta + \gamma\right), \quad i \in [N]
\end{align*}

That being said, there is a polynomial $Z \in \mathbb{F}[X]$ such that $\forall
d \in \mathcal{D}$ we have $Z(\omega^{0}) = 1$ and $Z(d)f(d) = g(d)Z(\omega d)$,
implying $A = B$ with overwhelming probability. That being said, we now can
encode our program using 8 polynomials mentioned at the very beginning:
\[q_L, q_R, q_M, q_O, q_C, S_{\sigma,1}, S_{\sigma,2}, S_{\sigma,3}\]

These are typically called \textbf{common preprocessed input}.

\subsubsection{Summary}
Having a program for relation $\mathcal{R}$, we saw how it can be represented as
a sequence of gates with left, right operands and output. The circuit may be
encoded using two matrices $Q$ --- for capturing gates, and $V$ --- for encoding
value carries (\textit{wirings}). Upon execution, we get trace execution matrix
$T$ and $\Pi$ for public inputs.

\begin{definition}
Let $T \in \mathbb{F}^{N \times 3}$ be a trace matrix with columns $\mathbf{A}$,
$\mathbf{B}$, $\mathbf{C} \in \mathbb{F}^N$ and let $\boldsymbol{\Pi} \in
\mathbb{F}^N$ be a public input vector. They correspond to a valid execution
instance with public input given by $\boldsymbol{\Pi}$ if and only if:
\begin{enumerate}
    \item \(\forall i \in [N]: A_i(Q_{L})_i + B_i(Q_{R})_i + A_iB_i(Q_{M})_i + C_i(Q_{O})_i + (Q_{C})_i + \Pi_i = 0\)
    \item \(\forall (i, j), \, \forall (k, \ell): V_{i,j} = V_{k,\ell} \implies T_{i,j} = T_{k,\ell}\)
    \item \(\forall i > n: \Pi_i = 0\), where $n$ is the number of public inputs.
\end{enumerate}
\end{definition}

Then, we encode these conditions in terms of polynomials.

\begin{definition}
Let $z_{\Omega} = X^N - 1$ be a vanishing polynomial. Let $T \in \mathbb{F}^{N
\times 3}$ be a trace matrix with columns $\mathbf{A}, \mathbf{B}, \mathbf{C}
\in \mathbb{F}^N$ and $\boldsymbol{\Pi} \in \mathbb{F}^N$ be a vector of public
signals. They correspond to a valid execution instance with public input given
by $\boldsymbol{\Pi}$ if and only if:

\begin{enumerate}
    \item \(\exists t_1 \in \mathbb{F}[X]: aq_L + bq_R + abq_M + cq_O + q_C + \pi = z_{\Omega}t_1\)
    \item \(\exists t_2, t_3, z \in \mathbb{F}[X]: zf - gz' = z_{\Omega}t_2\) and \((z-1)L_1 = z_{\Omega}t_3\), where $z'(X) = z(X\omega)$.
\end{enumerate}
\end{definition}
\begin{remark}
We can reduce every needed check down to one equation, if we introduce randomness.
Let $\alpha$ be a random field element, then: 
\[
    \begin{aligned}
        z_{\Omega}t &= aq_L + bq_R + abq_M + cq_O + q_C + \pi \\
        &= \alpha(gz' - fz) \\
        &= \alpha^2(z - 1)L_1
    \end{aligned}
\]

The transition between second and third line is very unobvious and requires a 
bit of algebraic manipulation. Don't worry if you don't see it immediately.
\end{remark}


\subsection{Plonk Prover and Verifier}

In this part, we will observe how the Plonk turns its arithmetization system
into the non-interactive proof. We consider how the prover and verifier specifically interact
in the protocol.

As in the previous sections, let $N$ be the size of our program (number of
gates). Let $\omega$ be a primitive $N$-th root of unity. Let $\Omega =
\{\omega^j\}_{0 \leq j < N}$. Define $Z_{\Omega}(X) := X^N - 1$ --- the vanishing 
polynomial of $\Omega$.

Assume we have conducted arithmetization of this program, obtaining eight
polynomials. Now we will demonstrate commitment-scheme agnostic non-interactive
(Fiat-Shamir) algorithms for both prover and verifier.

\begin{remark}
    Although the further discussion would involve statements like ``the prover
    picks the random $\alpha,\beta$, \ldots'', it is important to note that
    these values are not actually chosen by the prover, but rather computed
    deterministically from the transcript using Fiat-Shamir heuristic. In case 
    you are not familiar with Fiat-Shamir heuristic, we recommend you to revisit
    the \Cref{section:zk}.
\end{remark}

\subsubsection{Gadgets}

Before diving into the protocol, let us introduce the gadgets that will be used.

\textcolor{blue!60!black}{\textbf{Commitment Scheme.}} We use the polynomial
commitment scheme to commit to the polynomials. Similarly to Groth16, one might
safely think of KZG commitment scheme to commit to the polynomials: for $p(x)
\in \mathbb{F}^{(\leq N)}[X]$, the commitment is $\mathsf{com}(p) =
g^{p(\tau)}$, which is evaluated using the powers of the generator $g$. The
opening procedure at point $\zeta \in \mathbb{F}$ of the commitment is simply
finding the encryption $g^{Q(\tau)}$ of the quotient polynomial $Q(X) :=
(p(X)-y)/(X-\zeta)$, where $y$ is the value of the polynomial at $\zeta$.
Finally, verification simply involves applying pairing. Note that other
commitment schemes might be used as well.

\textcolor{blue!60!black}{\textbf{Blindings.}} In the protocol, to achieve 
the HZVK property (honest verifier zero-knowledge), the prover must somehow 
conceal the polynomials he works with. Specifically, consider the polynomial 
$b(X)$ that is computed as the interpolated values from the trace table (namely, 
$b(\omega^j) = T_{j,1}$). However, the prover, by providing the opening $b(\zeta)$,
should not reveal any information on polynomial $b(X)$ itself. To achieve this, the prover
extends $b(X) \in \mathbb{F}^{(\leq N)}[X]$ to the higher-degree polynomial $\widetilde{b}(X)$
with the condition that $\widetilde{b}(\omega^j) = b(\omega^j) = T_{j,1}$. However, for higher 
degrees, there are multitude of choices for $\widetilde{b}(X)$, so we need to standardize 
this choice. 

Assume we extend $b(X)$ to $\widetilde{b}(X) \in \mathbb{F}^{(\leq N')}[X]$ of
higher degree $N'>N$. For convenience, assume $N' = N + \delta$ with $\delta \in
\mathbb{N}$. How can we construct such $\widetilde{b}(X)$? We can do this by
sampling random values $\gamma_0,\gamma_1,\dots,\gamma_{\delta} \xleftarrow{R} \mathbb{F}$ and 
letting:
\begin{equation*}
    \widetilde{b}(X) = b(X) + z_{\Omega}(X)\sum_{j=0}^{\delta}\beta_jX^j
\end{equation*}

Why does this work? Let us substitute any $\omega^i \in \Omega$:
\begin{equation*}
    \widetilde{b}(\omega^i) = b(\omega^i) + z_{\Omega}(\omega^i)\sum_{j=0}^{\delta}\beta_j\omega^{ij} = b(\omega^i).
\end{equation*}

Here, we use the fact that $z_{\Omega}(\omega^i) = 0$ for all $i \in [N]$ since $z_{\Omega}$ is, 
by definition, the vanishing polynomial of $\Omega \ni \omega^i$.

\subsubsection{Proving}
First, the prover computes execution trace table $T \in \mathbb{F}^{N\times 3}$.
Then,  Let us 
finally proceed with the protocol.

\textcolor{green!60!black}{\textbf{Round 1.}} Add to the transcript commitments
of 8 arithmetizational polynomials:
\begin{equation*}
    \mathsf{com}(S_{\sigma,1}), \mathsf{com}(S_{\sigma,2}), \mathsf{com}(S_{\sigma,3}), \mathsf{com}(q_L), \mathsf{com}(q_R), \mathsf{com}(q_M), \mathsf{com}(q_O), \mathsf{com}(q_C)
\end{equation*}
Interpolate polynomials $a', b', c'$ over corresponding columns of $T$ matrix at
the domain $\Omega$. Sample random $b_1, b_2, b_3, b_4, b_5, b_6 \xleftarrow{R}
\mathbb{F}$. Let the blinded polynomials be:
\begin{align*}
\centering
a &:= (b_1 X + b_2)Z_{\Omega}(X) + a'(X) \\
b &:= (b_3 X + b_4)Z_{\Omega}(X) + b'(X) \\
c &:= (b_5 X + b_6)Z_{\Omega}(X) + c'(X)
\end{align*}
Add to the transcript commitments of computed above polynomials:
\begin{equation*}
    \mathsf{com}(a), \mathsf{com}(b), \mathsf{com}(c)
\end{equation*}

\textcolor{green!60!black}{\textbf{Round 2.}}  Sample random scalars $\beta,
\gamma \xleftarrow{R} \mathbb{F}$ from the transcript. Let $z_0 = 1$ and define
recursively for $k = 0, \dots, N$:
\begin{equation*}
    z_{k+1} = z_k \times \frac{(a_k + \beta \omega^{k} + \gamma) (b_k + \beta \omega^{k}k_1 + \gamma) (c_k + \beta \omega^{k}k_2 + \gamma)}{(a_k + \beta S_{\sigma,1}(\omega^{k}) + \gamma) (b_k + \beta S_{\sigma,2}(\omega^{k}) + \gamma) (c_k + \beta S_{\sigma,3}(\omega^{k}) + \gamma)}     
\end{equation*}

Interpolate polynomial $z'$ over evaluations $(z_0, \dots, z_{N-1})$ at the domain $\Omega$.

Sample random $b_7, b_8, b_9 \xleftarrow{R} \mathbb{F}$. Let $z = (b_7 X^2 + b_8
X + b_9)Z_H + z'$. Add to the transcript $\mathsf{com}(z)$.

\textcolor{green!60!black}{\textbf{Round 3}} Sample $\alpha \xleftarrow{R}
\mathbb{F}$ from the transcript. Let $\pi(X)$ be the interpolation polynomial of the
input matrix $\Pi$ at the domain $\Omega$. Let
\begin{align*}
p_1 &= aq_L + bq_R + abq_M + cq_o + q_C + \pi \\
p_2 &= (a + \beta X + \gamma)(b + \beta k_1 X + \gamma)(c + \beta k_2 X + \gamma)z - \\
    &- (a + \beta S_{\sigma,1} + \gamma)(b + \beta S_{\sigma,2} + \gamma)(c + \beta S_{\sigma,3} + \gamma)z(\omega X) \\
p_3 &= (z - 1)L_1
\end{align*}

Define the composite polynomial $p = p_1 + \alpha p_2 + \alpha^2 p_3$. Compute
$t$ such that $p = tZ_{\Omega}$. Write $t = t_{\text{lo}} +
X^{N+2}t_{\text{mid}} + X^{2(N+2)}t_{\text{hi}}$, with $t_{\text{lo}},
t_{\text{mid}}$, and $t_{\text{hi}} \in \mathbb{F}^{\leq (N+1)}[X]$ polynomials
of degree at most $N+1$. Sample random $b_{10}, b_{11} \xleftarrow{R} \mathbb{F}$ and define:
\begin{align*}
t_{\text{lo}} &= t'_{\text{lo}} + b_{10}X^{N+2} \\
t_{\text{mid}} &= t'_{\text{mid}} - b_{10} + b_{11}X^{N+2} \\
t_{\text{hi}} &= t'_{\text{hi}} - b_{11}
\end{align*}

Add to the transcript commitments: com($t_{\text{lo}}$), com($t_{\text{mid}}$), com($t_{\text{hi}})$.

\textcolor{green!60!black}{\textbf{Round 4}} Sample random $\zeta \xleftarrow{R}
\mathbb{F}$ from the transcript. Compute:
\[\bar{a} = a(\zeta), \bar{b} = b(\zeta), \bar{c} = c(\zeta), \bar{S}_{\sigma,1} = S_{\sigma,1}(\zeta), \bar{S}_{\sigma,2} = S_{\sigma,2}(\zeta), \bar{z}_{\omega} = z(\zeta \omega)\]
and add them to the transcript.

\textcolor{green!60!black}{\textbf{Round 5}} Sample random $v \xleftarrow{R}
\mathbb{F}$ from the transcript. Let:
\begin{align*}
\hat{p}_{nc1} &= \bar{a}q_L + \bar{b}q_R + \bar{a}\bar{b}q_M + \bar{c}q_o + q_C \\
\hat{p}_{nc2} &= (\bar{a} + \beta \zeta_1 + \gamma)(\bar{b} + \beta k_1 \zeta_1 + \gamma)(\bar{c} + \beta k_2 \zeta_1 + \gamma) z -\\
&- (\bar{a} + \beta \bar{S}_{\sigma_1} + \gamma)(\bar{b} + \beta \bar{S}_{\sigma_2} + \gamma)(\bar{c} + \beta \bar{S}_{\sigma_3} + \gamma) z(\omega \zeta_1) \\
\hat{p}_{nc3} &= L_1(\zeta_1)z
\end{align*}
Define:
\begin{align*}
p_{nc} &= p_{nc1} + \alpha p_{nc2} + \alpha^2 p_{nc3} \\
t_{partial} &= t_{lo} + \zeta^{N+2}t_{mid} + \zeta^{2(N+2)}t_{hi}
\end{align*}
The subscript $nc$ stands for "nonconstant," as it is the part of the linearization of $p$ with nonconstant factors. The subscript $partial$ indicates that it is a partial evaluation of $t$ at $\zeta$. Partial means that only some power of $X$ is replaced by the powers of $\zeta$. So in particular $t_{partial}(\zeta) = t(\zeta)$.
Let $\pi_{batch}$ be the opening proof at $\zeta$ of the polynomial $f_{batch}$ defined as: 
\[t_{partial} + v p_{nc} + v^2 a + v^3 b + v^4 c + v^5 S_{o1} + v^6 S_{o2}\]
Let $\pi_{single}$ be the opening proof at $\zeta\omega$ of the polynomial $z$. \\
Compute $\bar{p}_{nc} := p_{nc}(\zeta)$ and $\bar{t} = t(\zeta)$.

\paragraph{Proof}
\[com(a), com(b), com(c), com(z), com(t_{lo}), com(t_{mid}), com(t_{hi}),\] 
\[\bar{a}, \bar{b}, \bar{c}, \bar{S}_{o1}, \bar{S}_{o2}, \bar{z}_w, T_{batch}, T_{single}, \bar{p}_{nc}, \bar{t}\]

\subsubsection{Verification}

\paragraph{Transcript Initialization}

Same as prover, verified adds the following to the transcript:

\begin{center}
com($S_{o1}$), com($S_{o2}$), com($S_{o3}$), com($q_L$), com($q_R$), com($q_M$), com($q_o$), com($q_C$)
\end{center}

\paragraph{Extraction of values and commitments}

Firstly, the verifier needs to compute all the challenges. For that, he follows these steps:
\begin{itemize}
    \item Add com($a$), com($b$), com($c$) to the transcript.
    \item Sample two challenges $\beta$, $\gamma$.
    \item Add com($z$) to the transcript.
    \item Sample a challenge $\alpha$.
    \item Add com($t_{lo}$), com($t_{mid}$), com($t_{hi}$) to the transcript.
    \item Sample a challenge $\zeta$.
    \item Add $\bar{a}$, $\bar{b}$, $\bar{c}$, $\bar{S}_{o1}$, $\bar{S}_{o2}$, $\bar{z}_w$ to the transcript.
    \item Sample a challenge $v$.
\end{itemize}

\paragraph{Compute $pi(\zeta)$}
Also, he needs to compute a few values of all these data. First, he computes the $\Pi$ matrix with the public inputs and outputs. He needs to compute $pi(\zeta)$, where $pi$ is the interpolation of $\Pi$ at the domain $H$. But he doesn't need to compute pi. He can instead compute $pi(\zeta)$ as:
\[\sum_{i=0}^{n} L_{i}(\zeta) P_{I_i}\]
where $n$ is the number of public inputs and $L_i$ is the Lagrange basis at the domain H.

\paragraph{Compute claimed values $p(\zeta)$ and $t(\zeta)$}
Compute: 
\[\bar{p}_c = p_1(\zeta) + \alpha z_{w} \left( \bar{c} + \gamma \right) \left( \bar{a} + \beta \bar{S}_{\sigma_1} + \gamma \right) \left( \bar{b} + \beta \bar{S}_{\sigma_2} + \gamma \right) - \alpha^2 L_1(\zeta)\]
This is the constant part of the linearization of $p$. So, adding it to what the prover claims to be $\bar{p}_{nc}$, he obtains $p(\zeta) = \bar{p}_c + \bar{p}_{nc}$.

\paragraph{Compute com($t_{partial}$) and com($p_{nc}$)}
He computes these of the commitments in the proof as follows:
\[com(t_{partial}) = com(t_{lo}) + \zeta^{N+2}com(t_{mid}) + \zeta^{2(N+2)}com(t_{hi})\]
For the second one, compute those:
\begin{align*}
\hat{p}_{nc1} &= \bar{a} * com(q_L) + \bar{b} * com(q_R) + (\bar{a}\bar{b}) * com(q_M) + \bar{c} * com(q_o) + com(q_C) \\
\hat{p}_{nc2} &= (\bar{a} + \beta \zeta_1 + \gamma)(\bar{b} + \beta k_1 \zeta_1 + \gamma)(\bar{c} + \beta k_2 \zeta_1 + \gamma) * com(z) - \\ 
&- (\bar{a} + \beta \bar{S}_{\sigma_1} + \gamma)(\bar{b} + \beta \bar{S}_{\sigma_2} + \gamma)(\bar{c} + \beta \bar{S}_{\sigma_3} + \gamma) * com(z)(\omega \zeta_1) \\
\hat{p}_{nc3} &= L_1(\zeta_1) * com(z)
\end{align*}
Then: \[com(p_{nc}) = com(p_{nc1}) + com(p_{nc2}) + com(p_{nc3})\].

\paragraph{Compute claimed value $f_{batch}(\zeta)$ and $com(f_{batch})$}

\begin{align*}
f_{batch}(\zeta) &= \bar{t} + v \bar{p}_{nc} + v^2 \bar{a} + v^3 \bar{b} + v^4 \bar{c} + v^5 \bar{S}_{o1} + v^6 \bar{S}_{o2} \\
com(f_{batch}) &= com(t_{partial}) + v * com(p_{nc}) + v^2 * com(a) + \\
&+ v^3 * com(b) + v^4 * com(c) + v^5 * com(S_{o1}) + v^6 * com(S_{o2})
\end{align*}
\paragraph{Proof check}

Now the verifier has all the necessary values to proceed with the checks.

\begin{itemize}
    \item Check that $p(\zeta)$ equals $(\zeta^N - 1)t(\zeta)$.
    \item Verify the opening of $f_{batch}$ at $\zeta$. That is, check that 
        \begin{equation*}
            \text{Verify}([f_{batch}], \pi_{batch}, \zeta, f_{batch}(\zeta)) \text{ outputs Accept.}
        \end{equation*}
    \item Verify the opening of $z$ at $\zeta_w$. That is, check the validity of the proof $\pi_{single}$ using the commitment $com(z)$ and the value $\bar{z}_w$. 
        \begin{equation*}
            \text{That is, check that } \text{Verify}(com(z), \pi_{single}, \zeta_w, \bar{z}_w) \text{ outputs Accept.}
        \end{equation*}
\end{itemize}

If all checks pass, output Accept. Otherwise, output Reject.

\end{document}